apiVersion: batch/v1
kind: Job
metadata:
  name: pytorch-gpu-demo
  namespace: ai-workloads
  labels:
    app: pytorch-gpu-demo
    app.kubernetes.io/part-of: gpu-samples
    app.kubernetes.io/managed-by: argocd
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: pytorch-gpu-demo
    spec:
      restartPolicy: Never
      containers:
        - name: demo
          # Simple, public GPU-enabled image
          image: nvcr.io/nvidia/pytorch:24.10-py3
          command:
            - bash
            - -lc
            - |
              echo "ðŸ”¢ Checking CUDA availability in PyTorch..."
              python - << 'PYCODE'
              import torch
              print("torch.version:", torch.__version__)
              print("cuda available:", torch.cuda.is_available())
              print("device count:", torch.cuda.device_count())
              if torch.cuda.is_available():
                  print("device:", torch.cuda.get_device_name(0))
              else:
                  print("ðŸš« No GPU detected by PyTorch yet.")
              PYCODE
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
              nvidia.com/gpu: 1    # requires a GPU node to actually schedule
            limits:
              cpu: "2"
              memory: "4Gi"
              nvidia.com/gpu: 1
          # OPTIONAL: later youâ€™ll add nodeSelector / tolerations to target GPU nodes
          # nodeSelector:
          #   nvidia.com/gpu.present: "true"
